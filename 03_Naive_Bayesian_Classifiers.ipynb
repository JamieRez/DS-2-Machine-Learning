{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Naive Bayesian Classifiers</h1></center>\n",
    "\n",
    "In this notebook, we'll be focusing on **_Naive Bayesian Classifiers_**, or \"Naive Bayes\" for short.  This is an algorithm that uses **_Bayes' Theorem_** to make a classification based on probability.  In case you're unfamiliar with Bayes' Theorem, let's look at the formula:\n",
    "<br>\n",
    "<br>\n",
    "<center><img src='img/bayes_theorem.png' height=40% width=40%></center>\n",
    "<br>\n",
    "<br>\n",
    "Don't worry if you've seen this mathematical notation before. In plain English, that formula reads:\n",
    "\n",
    "\"The probability of A given B equals the probability of B given A, times the probability of A, divided by the probability of B\".  \n",
    "\n",
    "Let's run through an example case here and see if we can demystify this equation a little bit more. \n",
    "\n",
    "<center><h3>Scenario: Spam Detection</h3></center>\n",
    "\n",
    "We have a dataset of emails, and we're trying to build a classifier that can predict if an email is spam or not by examining the words based in the emails.  Each email in our training set has been labeled as \"spam\" or \"ham\" (a real email, not spam).  We've counted each word used in every email, and found the following:\n",
    "\n",
    "**_65% of the emails in the dataset are \"Spam\"._**\n",
    "\n",
    "**_\"Spam\"_** emails contain the word _\"deal\"_ 80% of the time, and _\"win\"_ 40% of the time.  \n",
    "\n",
    "**_35% of the emails in the datasert are \"Ham\"._**\n",
    "\n",
    "**_\"Ham\"_** emails contain the word _\"deal\"_ 17% of the time, and _\"win\"_ 6% of the time.  \n",
    "\n",
    "The next email we try to predict contains the both words \"deal\" and \"win\". Given the information above, we can plug these numbers into Bayes' Theorem and predict the likelihood that this is email Spam. \n",
    "\n",
    "<center>P(Spam|deal, win) = (P(win, deal|Spam) * P(Spam)) / P(deal, win)</center>\n",
    "\n",
    "This can be further broken down into: \n",
    "<br>\n",
    "<br>\n",
    "<center>P(Spam|deal) \\* P(Spam|win) = P(deal|Spam) \\* P(Spam) \\*  P(win|Spam) \\* P(Spam) / P(deal|Spam) + P(deal|!Spam) \\* P(win|Spam) + P(win|!Spam)</center>\n",
    "\n",
    "In the equation above, \"P(deal|!Spam)\" can be read as \"the percentage that 'deal' occurs in 'Ham' emails\".  \n",
    "\n",
    "On the next step, we'll start defining the probabilities for everything in that equation so we can plug them in:\n",
    "\n",
    "1. P(deal|Spam) = .8\n",
    "1. P(win|Spam) = .4\n",
    "1. P(Spam) = .65\n",
    "1. P(deal|!Spam) = .17\n",
    "1. P(win|!Spam) = .06\n",
    "1. P(!Spam) = .35\n",
    "\n",
    "Let's replace some of these terms with the probabilities listed above and see how it works out:\n",
    "<br>\n",
    "<br>\n",
    "<center>(.8 \\* .65 \\* .4 \\* .65) / .8 \\* .65 + .35 \\* .17 \\* .4 \\* .65 + .35 \\* .6 = **0.922595** </center>\n",
    "<br>\n",
    "<br>\n",
    "Based on the math from Bayes' Theorem, we can predict probability that a new email containing both \"deal\" and \"win\" is \"Spam\" is approximately **92.2%**!\n",
    "\n",
    "<center><h3>Using Naive Bayes in the Real World</h3></center>\n",
    "\n",
    "In the above example, we did the math by hand.  That isn't very practical in the real world.  Luckily, `sklearn` contains some awesome implementations of Naive Bayesian Classifiers (and regressors!).  \n",
    "\n",
    "For this assignment, we're going to use a `GaussianNB()` object.  There are a few different kinds of Naive Bayesian Classifiers, but for this one we'll stick to one that assumes our data follows a Gaussian (normal) distribution.  \n",
    "\n",
    "Let's Get Started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n",
      "accuracy score: 1.0\n",
      "f1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Use load_iris() to load the data into the iris variable, and then assign iris.data and iris.target to the appropriate\n",
    "# variables\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "# Use train_test_split to split the data into X_train, X_test, y_train, and y_test variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "# Create a GaussianNB() object and fit it using the training data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted model to create predictions for the X_test data.\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Run it all and see how you did!\n",
    "print(preds)\n",
    "print(y_test)\n",
    "print(\"accuracy score: {}\".format(accuracy_score(y_test, preds)))\n",
    "print(\"f1 score: {}\".format(f1_score(y_test, preds, average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Some Caveats</h3></center>\n",
    "\n",
    "You may have wondered why this particular model is a called a **_Naive_** Bayesian Classifier.  In this scenario, the word \"Naive\" simply means that the model makes the \"naive\" assumption that all features are independent of one another.  This leads us to the main caveat of this model--if you have feature columns that are highly correlated, this model may not work as well as we'd like.  **_If you're going to use Naive Bayes, make sure you check for highly correlated features beforehand!_**\n",
    "\n",
    "\n",
    "<center><h3>Where to Go From Here</h3></center>\n",
    "\n",
    "For the latter part of this assignment, you're going to use the famous Pima Indians Diabetes Dataset to build a Naive Bayesian Classifier that predicts whether or not an individual has diabetes.  You'll find the `pima_indians_diabetes.csv` file inside the `datasets` folder.  \n",
    "\n",
    "To build this classifier successfully, you'll want to follow the best practices for loading in and preprocessing a data set that you've learned in class:\n",
    "\n",
    "1. Importing the data\n",
    "1. Exploring the data\n",
    "1. \"Cleaning\" the data\n",
    "1. Splitting the data into training and testing sets (or using KFold Cross val--more on this below)\n",
    "1. Fitting the model\n",
    "1. Validating the model (checking predictions on the test set)\n",
    "\n",
    "Be sure to consider the following questions as you solve this problem:\n",
    "\n",
    "* How will you deal with null values?\n",
    "* For this model, does scaling the data improve your results? (HINT: test your assumption!)\n",
    "\n",
    "On top of cleaning and preprocessing this data set, you'll also use **_Cross Validation_** to get a better measure of the accuracy of your model.  We did not use K Fold Cross Validation in the above model on purpose--instead, you'll need to work your way through `sklearn`'s [model_selection documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) to figure out how to effectively make use of cross-validation.  \n",
    "\n",
    "(**_Hint:_** There are several ways to implement cross validation using sklearn.  In the `model_selection` section of the documentation, pay special attention to the `KFold` object, as well as the methods available under the _Model Selection_ subsection.)\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>0.851</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>0.267</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.966</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>1.390</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.271</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>45.4</td>\n",
       "      <td>0.721</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>0.491</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.526</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.342</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0.467</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.718</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>7.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>0.254</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>0.699</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>0.334</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.189</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.867</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>0.411</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>0.231</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>4.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0.598</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>0.483</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.118</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>2.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>34.4</td>\n",
       "      <td>0.176</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.674</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>3.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>0.295</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>5.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.439</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>7.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>33.9</td>\n",
       "      <td>0.826</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>3.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>33.8</td>\n",
       "      <td>0.970</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.415</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>1.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>0.289</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>1.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.349</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>5.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>39.1</td>\n",
       "      <td>0.251</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>36.1</td>\n",
       "      <td>0.496</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.323</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>44.5</td>\n",
       "      <td>0.646</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>2.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.426</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "3            1.0     89.0           66.0           23.0     94.0  28.1   \n",
       "6            3.0     78.0           50.0           32.0     88.0  31.0   \n",
       "8            2.0    197.0           70.0           45.0    543.0  30.5   \n",
       "13           1.0    189.0           60.0           23.0    846.0  30.1   \n",
       "14           5.0    166.0           72.0           19.0    175.0  25.8   \n",
       "18           1.0    103.0           30.0           38.0     83.0  43.3   \n",
       "19           1.0    115.0           70.0           30.0     96.0  34.6   \n",
       "20           3.0    126.0           88.0           41.0    235.0  39.3   \n",
       "24          11.0    143.0           94.0           33.0    146.0  36.6   \n",
       "25          10.0    125.0           70.0           26.0    115.0  31.1   \n",
       "27           1.0     97.0           66.0           15.0    140.0  23.2   \n",
       "28          13.0    145.0           82.0           19.0    110.0  22.2   \n",
       "31           3.0    158.0           76.0           36.0    245.0  31.6   \n",
       "32           3.0     88.0           58.0           11.0     54.0  24.8   \n",
       "35           4.0    103.0           60.0           33.0    192.0  24.0   \n",
       "39           4.0    111.0           72.0           47.0    207.0  37.1   \n",
       "40           3.0    180.0           64.0           25.0     70.0  34.0   \n",
       "43           9.0    171.0          110.0           24.0    240.0  45.4   \n",
       "50           1.0    103.0           80.0           11.0     82.0  19.4   \n",
       "51           1.0    101.0           50.0           15.0     36.0  24.2   \n",
       "52           5.0     88.0           66.0           21.0     23.0  24.4   \n",
       "53           8.0    176.0           90.0           34.0    300.0  33.7   \n",
       "54           7.0    150.0           66.0           42.0    342.0  34.7   \n",
       "56           7.0    187.0           68.0           39.0    304.0  37.7   \n",
       "63           2.0    141.0           58.0           34.0    128.0  25.4   \n",
       "68           1.0     95.0           66.0           13.0     38.0  19.6   \n",
       "69           4.0    146.0           85.0           27.0    100.0  28.9   \n",
       "70           2.0    100.0           66.0           20.0     90.0  32.9   \n",
       "71           5.0    139.0           64.0           35.0    140.0  28.6   \n",
       "73           4.0    129.0           86.0           20.0    270.0  35.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "698          4.0    127.0           88.0           11.0    155.0  34.5   \n",
       "700          2.0    122.0           76.0           27.0    200.0  35.9   \n",
       "704          4.0    110.0           76.0           20.0    100.0  28.4   \n",
       "707          2.0    127.0           46.0           21.0    335.0  34.4   \n",
       "709          2.0     93.0           64.0           32.0    160.0  38.0   \n",
       "710          3.0    158.0           64.0           13.0    387.0  31.2   \n",
       "711          5.0    126.0           78.0           27.0     22.0  29.6   \n",
       "715          7.0    187.0           50.0           33.0    392.0  33.9   \n",
       "716          3.0    173.0           78.0           39.0    185.0  33.8   \n",
       "718          1.0    108.0           60.0           46.0    178.0  35.5   \n",
       "721          1.0    114.0           66.0           36.0    200.0  38.1   \n",
       "722          1.0    149.0           68.0           29.0    127.0  29.3   \n",
       "723          5.0    117.0           86.0           30.0    105.0  39.1   \n",
       "726          1.0    116.0           78.0           29.0    180.0  36.1   \n",
       "730          3.0    130.0           78.0           23.0     79.0  28.4   \n",
       "732          2.0    174.0           88.0           37.0    120.0  44.5   \n",
       "733          2.0    106.0           56.0           27.0    165.0  29.0   \n",
       "738          2.0     99.0           60.0           17.0    160.0  36.6   \n",
       "740         11.0    120.0           80.0           37.0    150.0  42.3   \n",
       "741          3.0    102.0           44.0           20.0     94.0  30.8   \n",
       "742          1.0    109.0           58.0           18.0    116.0  28.5   \n",
       "744         13.0    153.0           88.0           37.0    140.0  40.6   \n",
       "745         12.0    100.0           84.0           33.0    105.0  30.0   \n",
       "747          1.0     81.0           74.0           41.0     57.0  46.3   \n",
       "748          3.0    187.0           70.0           22.0    200.0  36.4   \n",
       "751          1.0    121.0           78.0           39.0     74.0  39.0   \n",
       "755          1.0    128.0           88.0           39.0    110.0  36.5   \n",
       "760          2.0     88.0           58.0           26.0     16.0  28.4   \n",
       "763         10.0    101.0           76.0           48.0    180.0  32.9   \n",
       "765          5.0    121.0           72.0           23.0    112.0  26.2   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  \n",
       "3                       0.167   21  \n",
       "6                       0.248   26  \n",
       "8                       0.158   53  \n",
       "13                      0.398   59  \n",
       "14                      0.587   51  \n",
       "18                      0.183   33  \n",
       "19                      0.529   32  \n",
       "20                      0.704   27  \n",
       "24                      0.254   51  \n",
       "25                      0.205   41  \n",
       "27                      0.487   22  \n",
       "28                      0.245   57  \n",
       "31                      0.851   28  \n",
       "32                      0.267   22  \n",
       "35                      0.966   33  \n",
       "39                      1.390   56  \n",
       "40                      0.271   26  \n",
       "43                      0.721   54  \n",
       "50                      0.491   22  \n",
       "51                      0.526   26  \n",
       "52                      0.342   30  \n",
       "53                      0.467   58  \n",
       "54                      0.718   42  \n",
       "56                      0.254   41  \n",
       "63                      0.699   24  \n",
       "68                      0.334   25  \n",
       "69                      0.189   27  \n",
       "70                      0.867   28  \n",
       "71                      0.411   26  \n",
       "73                      0.231   23  \n",
       "..                        ...  ...  \n",
       "698                     0.598   28  \n",
       "700                     0.483   26  \n",
       "704                     0.118   27  \n",
       "707                     0.176   22  \n",
       "709                     0.674   23  \n",
       "710                     0.295   24  \n",
       "711                     0.439   40  \n",
       "715                     0.826   34  \n",
       "716                     0.970   31  \n",
       "718                     0.415   24  \n",
       "721                     0.289   21  \n",
       "722                     0.349   42  \n",
       "723                     0.251   42  \n",
       "726                     0.496   25  \n",
       "730                     0.323   34  \n",
       "732                     0.646   24  \n",
       "733                     0.426   22  \n",
       "738                     0.453   21  \n",
       "740                     0.785   48  \n",
       "741                     0.400   26  \n",
       "742                     0.219   22  \n",
       "744                     1.174   39  \n",
       "745                     0.488   46  \n",
       "747                     1.096   32  \n",
       "748                     0.408   36  \n",
       "751                     0.261   28  \n",
       "755                     1.057   37  \n",
       "760                     0.766   22  \n",
       "763                     0.171   63  \n",
       "765                     0.245   30  \n",
       "\n",
       "[336 rows x 8 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to file: \"datasets/pima_indians_diabetes.csv\". The first row of the .csv contains the column names.\n",
    "# Note that in the \"Outcome\" column, 0 denotes someone that does NOT have diabetes, and 1 denotes someone that does.  \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "df = pd.read_csv(\"datasets/pima_indians_diabetes.csv\")\n",
    "\n",
    "outcomes = df[\"Outcome\"]\n",
    "data = df.drop(\"Outcome\", axis=1)\n",
    "# Get rid of those 0 values\n",
    "for colName in data.columns.values:\n",
    "    data[colName] = data[data[colName] > 0][colName]\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 1\n",
      "accuracy score: 0.4264705882352941\n",
      "f1 score: 0.42834119949349597\n",
      "\n",
      "Split: 2\n",
      "accuracy score: 0.4626865671641791\n",
      "f1 score: 0.4757356076759063\n",
      "\n",
      "Split: 3\n",
      "accuracy score: 0.582089552238806\n",
      "f1 score: 0.5772350369365296\n",
      "\n",
      "Split: 4\n",
      "accuracy score: 0.5223880597014925\n",
      "f1 score: 0.5146552949538024\n",
      "\n",
      "Split: 5\n",
      "accuracy score: 0.5970149253731343\n",
      "f1 score: 0.5792753035993912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "curSplit = 1\n",
    "for train_index, test_index in kf.split(data):\n",
    "    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "    y_train, y_test = outcomes[train_index], outcomes[test_index]\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    print(\"Split: {}\".format(curSplit))\n",
    "    print(\"accuracy score: {}\".format(acc))\n",
    "    print(\"f1 score: {}\\n\".format(f1))\n",
    "    curSplit += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 1\n",
      "accuracy score: 0.4264705882352941\n",
      "f1 score: 0.42834119949349597\n",
      "\n",
      "Split: 2\n",
      "accuracy score: 0.4626865671641791\n",
      "f1 score: 0.4757356076759063\n",
      "\n",
      "Split: 3\n",
      "accuracy score: 0.582089552238806\n",
      "f1 score: 0.5772350369365296\n",
      "\n",
      "Split: 4\n",
      "accuracy score: 0.5223880597014925\n",
      "f1 score: 0.5146552949538024\n",
      "\n",
      "Split: 5\n",
      "accuracy score: 0.5970149253731343\n",
      "f1 score: 0.5792753035993912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "curSplit = 1\n",
    "for train_index, test_index in kf.split(scaled_data):\n",
    "    X_train, X_test = scaled_data[train_index], scaled_data[test_index]\n",
    "    y_train, y_test = outcomes[train_index], outcomes[test_index]\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    print(\"Split: {}\".format(curSplit))\n",
    "    print(\"accuracy score: {}\".format(acc))\n",
    "    print(\"f1 score: {}\\n\".format(f1))\n",
    "    curSplit += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
