{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Regression</h1></center>\n",
    "\n",
    "Of all the techniques in Machine Learning and Predictive Analytics, **_Regression_** is one of the most mature.  Recall that regression is different from the classification problems we have dealt with so far; whereas classification looks at data and tells us what class it belongs to, regression instead seeks to output a continuous value.  Classification and regression are often two sides of the same coin, and which one you use often depends on the phrasing of the question you're trying to answer.  For this exercise, we'll be trying to predict housing prices on the `boston_housing_prices` data set.  Let's look at an some example questions we could ask about this data set.\n",
    "\n",
    "<h3>Classification</h3>\n",
    "* Will this house sell for more or less than this price? *\n",
    "<br>\n",
    "* Does this house have less than 3 offers on it? *\n",
    "<br>\n",
    "* Based on the price and other data about the house, can we predict if this house has 2 bedrooms, or less, or more?*\n",
    "\n",
    "<h3>Regression</h3>\n",
    "* How much do we think this house will sell for? *\n",
    "<br>\n",
    "* Based on the data, how many offers do we expect this house to receive? *\n",
    "<br>\n",
    "* Based on price and other data about the house, can we predict how many bedrooms a house has? *\n",
    "<br>\n",
    "<br>\n",
    "As you can see, these are pretty much the same question, but phrased differently.  Under the hood, classification is often just a regression, with the caveat that values below the threshold are one class while values above the threshold are another.  \n",
    "\n",
    "<center><img src='img/classification_vs_regression.png' height=40% width=40%></center>\n",
    "\n",
    "<center><h2>The Basics: Linear Regression</h2></center>\n",
    "\n",
    "Before we begin coding, we'll briefly explore how it works and how we can interpret a regression.  This lesson will probably feel more math-heavy than other lessons, and will certainly contain more jargon--but don't worry! The math behind linear regression is fairly intuitive (and fairly simple), and regression is great baseline for understanding how more advanced topics in machine learning work, such as Neural Networks. \n",
    "\n",
    "<h3>The Idea Behind Regression</h3>\n",
    "\n",
    "The most basic form of regression is **_Linear Regression_**.  Linear Regression is a **_Linear Modeling_** technique used to predict a variable as a function of another variable. You've probably seen this written out like this before:\n",
    "\n",
    "<center><h3>y = f(x)</h3></center>\n",
    "\n",
    "The most common version of this function (and the one we're trying to discover in Linear Regression) is also one you're likely familiar with:\n",
    "\n",
    "<center><h3>y = mx + b</h3></center>\n",
    "\n",
    "This is the basic formula for a line on a cartesian plane that you likely learned in grade school.  In linear regression, our goal is to take some data: \n",
    "    \n",
    "\n",
    "<center><img src='img/scatter.png' height=40% width=40%></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>and discover the function for the **_line of best fit._**</center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src='img/lbf.png' height=60% width=60%></center>\n",
    "\n",
    "Let's take a look at the line for the equation and interpret what it means.  You'll often see the the equation for the line of best fit represented like this:\n",
    "\n",
    "<center><img src='img/regression_equation.png' height=40% width=40%></center>\n",
    "\n",
    "Let's examine what these symbols mean, and see if we can't \n",
    "\n",
    "**_α_**: This value is the bias.  Mathematically, we can think of this as where our line crosses the y-axis.  \n",
    "\n",
    "**_β_**: is the slope, which defines the relationship between x and y.  In layman's terms, we can interpret this value as how much we expect to y to increase whenever we increase the value of x by 1.  \n",
    "\n",
    "If we were trying to use linear regression to predict something like how much it would cost for a factory to produce \\[x\\] widgets, then we could use this equation to make a prediction. In this scenario:\n",
    "\n",
    "β would represent the cost per unit to produce a widget.  How much we have to spend on manufacturing depends on how many we want ot manufacture.  If we want to manufacture 10,000 widgets and they cost us $.80 each, then the value for β would be .8.\n",
    "\n",
    "α would represent the fixed costs that don't change with the scale of x. In this scenario, these could be fixed operating costs like how much rent we have to pay to occupy the factory.  There's no avoiding this cost, and it doesn't increase or decrease with our scale.  If the rent for our factory is $10,000/month, then we know we'll have to pay $10,000 this month, regardless of whether we make 1 widget or 1 million widgets.  \n",
    "\n",
    "\n",
    "<center><h3>Fitting a Linear Regression in Python</h3></center>\n",
    "\n",
    "In the code block below, we'll create a basic data set that contains some correlation between X and Y.  Then, we'll fit a linear regression to it, and display the line of best fit. Feel free to examine the code and try to figure out how it works, but don't worry if you don't understand everything just yet--this is just help us create a quick visualization of a line of best fit against a scatterplot of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnXeYFUXWh9/DkEaCA4KIAyyYUBAB\nHQQFEUEWXHEdMaGooCiGNSsumHBNoLLioqjLGmBFxV3kw7RiAlwEE8KKoEgyMIMKShAQJNX3R/Vg\nc+d2354b5t7b97zPc5+Z7qruPt3V/euqU6erxBiDoiiKEl6qpNsARVEUJbWo0CuKooQcFXpFUZSQ\no0KvKIoSclToFUVRQo4KvaIoSsjJKKEXkZYiMl9ENorI1c6634vI1IDbfyQirVNrpaIoSpZhjMmY\nH/AkMDpi3VygU8DtzwJeTJFt1wHfAxuAp4AaPnkvBpYBm4BpwP6utAJgArDa+d3hSmvmbOP+GeAG\nV56rgK+An51r08WVdgewPWL7A1zp3YF5zrYrgMGutJsjttsC7AIaOOmLItJ3AK9EOfcBjs0Xu9YN\nARYCGx3bh7jS9gWeB1Y513Y20NGVfjLwHrDeuf7/AOpElPkc4BdgZoQthwAvAWuAtcAbQEtXugB3\nA6XOsWcCrV3p9wMrnev1DXBLBjwj9YH/AzY7Np3rk/f1iDLbBnzmSm8OzHCu3WLgRFfaQGBnxPbd\nXOnHAh85Zbog4j5sDLzslKkBmkc5hxeAH53fs0DdIPeDk+dc59w3A1OB+hHp/YAvnPTlwHHO+lbY\nZ2ad83sbaBWx7ZHAf53z/QG4JuB92g37vLiv14Cg5Yb/c+1bjoHum3TfuBEn+zZ7CkQHYGkFtq+J\nfaAbJ9muXk6htwbqYQVhpEfe47EC3hqoDjwGvOtKfxr4N7CX86AtBy702FcL52Fr7ix3dG6Uo7Ai\ndTlWxPKc9DuAiR77qubcoJc623Zwbpq2HvnvAKZ7pAn2RXFBxPp6WMFYGFGONzkPUFWgpXOj93PS\nDgCux4pDHjAY+/DXdtLPBXo716uec9M/7tr3iVixv53yQn80MMh5yKoBdwGLXelnYR/cA5xjjwDm\nudJbArWc/wuxL7u+SbifZuISzQpu+zxWJGsDXZwybV2B497uWn4feBDIB07HvkwbOmkDgfc89lPf\nKaMznet2HlY46znpjYArgGOILvSPAm8CdYG9sc/9gwHvh9bYl0tX5xo8B0xy7bunc391wnosCoFC\nJ60A+8yJs++rgQWubRtgn93+QA2gDnBYQLu6ASXxlBsxnutY5Rio7BO9aZP1A6ZjRW0rVoAOwT68\nT7jyHOtc3KbOclvn5jzUlectXG/SJNn2HHCva7kH8L1H3lHAWNfy/s7NfqCz/CPQwZV+MzDLY1/D\ngRmu5bOBj1zLtZx9N3aW78Bb6Bs5efdyrfsYOCdKXsG+gKJeR+zLbBOOCLrWP459wGfiEvoo248B\nHvZJ/xk4yiOtL1FqM9hW1MwY5VjfuQb7OMt/Bv7lSm8NbPXYthD4DLjJVRYr+K0mehK2xdEwwP00\nkziE3invbcAhrnXP4FHpiNi2ufN8tXCWDwF+Zc/W0SzgMuf/gXgLfR9gUcS6JcCgiHVViS70rwNX\nuJb/BLwR5H4A7gWec6Ud6FyTOs7ynEg7PPZZ1TnuL6519wLPVKA83HZ1w0PoY5UbMZ5rv3IM+ssY\nH70xpjv2RrvSGFPbGLMEaAN86cozB/g7MEFE8rEX61ZjzGLXrr7AvgDKISJdRGS9z6+Lh3mtgU9d\ny58CjURkn2iHcX7uZYDDo6wr+9+d5uYCrJunjNeBPBHpKCJ5wEXA/7ACU8YpIrJWRBaJyOVlK40x\nP2BrFReKSJ6IHAP8DusWieQ47IvhRQ+7BgCTjTGbd5+EyNFAEVbsPRERcfa/yCO9HbYltMxjF129\ntg1AV+wL+idneRJwkIgcIiLVsOc1LcKeoSKyCSjBPoDPARhjXsDWiMc498GT2JfbmjhtC8IhwE7n\n2SjjU+z9GYsLsBWKr5zl1sAKY8xGn321F5EfRWSJiNwmIlWd9ZH3eNk6r/s4krFAHxGpJyL1sK2J\n16NljHI/7PEsGmOW44io80wUAQ1FZJmIlIjII45WuPe5HluhfBgr7mV0AtaKyBwRWS0ir4hIs4B2\nAewrIj+IyFciMlpEajnrY5VbkOe6jMhyDEZFaxWp/BFRE8TWzi+LyFMN+ARbu5oGSET6PcBTSbZr\nOdA7woZyNRUnrQe21n4Etkn8d6zv7hwnfSIwBdssPMjZ969R9nMcttZc27VOsC2A7VgfeWTroBW2\nBZGHbf18h6vGDpyCdUHtcH6XeJzvk8B4j7S9sDWZbq51eVi/4jHRyjFi+79gb/JyfRzYpvxnwDCP\nbXtiXQSHREnzrdEDTbC+ePf1qA78zSnLHVgfabmaknPd2zu2u2vABcC3js1/r+B93i1o/oh74vuI\ndZf4nbcr3zJgoGv5fOCDKM/OeOf/A7CuwyrYCtfnZeUC7INtSZ/jPAsDnHv87xH786rR74911+xy\nfm8B1YPcD8A7lNeEUmyNuqz1PBfrYmmA9aXfE2XftbCtz5Nd65Y459UB6wYeA8wOaNd+2OevinPd\n/lt2PWKVGzGea79yDHzvVHSDVP4oL/T/wtVx51p/lVOgPaOkjcHx9yXRrk+Bs1zL++ByAUTJ/ydg\nKdbfNwzrjyvrEKqP7Xz6HlszvRtYHmUfTwATotwcy7A1hCpY3/UPuDp7I/IPxemcBg7Fdrr1crZt\n6dh4csQ2+VghP8Fjn+cBX+N6wTrl8ZRreY9ydK2/EiumTaKk5QPvAv/wOG4nrN+yh0e6p9ADDbFC\ndUvE+nuwTf0mWFEa6Ni3l8d+hkbeW8BfnXuh3MsnIt96128H9iVetjzUYxt3J1x/7Mvml4g8NxCl\nUzwiTxfKVxpOAz6PyPcwHi41bAfnJ67l47Guv7XYluKbwG0R23gJ/Wysn74W1mf9OC4Xmt/9gO1c\nvyli3Uasf7uec7wBrrTTgfke51QF+AnY1/WcPx3lOd876H0acb/+5PzvW24EfK6jlWPQX8a4bjxY\ngD353YhIIdZ3/TTwVxGpEbHNYezpZnFve5yIbPL5HedhxyL2dAe1BX4wv7kA9sAYM9YYc7AxZl+s\n+6MqtoMSY8xaY0x/Y8x+xpjW2IL9KMLOfGxH14SIXbfF3hxLjDG7jDHTsLX2Yz3sNuzpOvrSGPOG\ns+2XwGtY37KbvtiHd6bHPgcA/zTOnefQAzhNRL4Xke8de/4qIo+4zukirFD2MMaURJxvDWz0RCm2\ns5iI9PbYKI6LjDHveNgVFcc18CbwsjHmnojktsALxpgSY8wOY8x4rFi08thdVaxPuGzf7bDN7Oex\nFQxPjDEFZT+su6yPa91Ij21OMtaNWdsY8yy2xllVRA6OOIdYrqwBwBRjzCbXukXAASJSJ+C+3PcS\nxph3jTEdjDH1sa2DlkTcxz60xdZ2Nzs2PQ78oSwxxv2wx7MoIgdgO06XGGPWYV1shmBUwbZQC53l\nBRHblv0vAeyKxH29YpVb0Oc6WjkGo6JvhlT+KF+jPxJbgGXLgm3m3ef8/wZwvyu9BlakotZwE7Cr\nN7YG3gorBNPxjrqpiRVVwYZLzmTPjtwDsTWFPKzI/khE1AS/hY9FuqUGODfNAc7+e2Jr6Yc66ac6\n9gk24qQUp3bjHHcTNsRSnOVlRLhvsKJ4p8e5NcHWRg+MWF+AbbqW/eZgIxT2dtL7O9fvsCj7rAa8\ngn2AqkZJPxxbuznbw6Y855pfhm0u1wSqOWl1seLziMe2w7Gi2wj70J+PjX4ocJYvjbie3wFXu8p5\nITZCoga2KX9FtON43Ofd4rwXJ2FfLLWAzsSIusHWQNcD3aOkfYANHqiJreG7o25OAho5/x/qnOtw\n17btnbKrCzxEhIvD2WdZp2JLoKYrbQa29ZDv/B4t2z7A/dAa2+I8ztn/RPaMurkT29LY1ym7WcBd\nTlpPx+48x+4x2Kirmk56d6xrsJ1jx2icQIkAdnXDPu8CNHXO8ekg5UaM5zpWOQa6b+LZKFU/ojT5\nnULr6Px/DfatW91Z3h/bnC9zi5yJfeOlwrbrsYLzM7Y1UcOVtgjo7/xf4Ni4GStuI3CFSfFbSN8v\n2A6XXlGO9UbZzRmxXpwb+Vtsc/UL4HxX+vPYpugmbJjj1RHbn8Vv8ewl2BdmFVd6IVbID/K4BsPw\niBDyK0esOyQyvv9xJ+14rBj8EpFeVqZPUz4+eZFr3wOd7d2/8a4HyDhl4d6+mZNeE9sx+J1TrvNw\n+mKwQj8NW3HYhH0Qb8Z5+WJFYJrLjrZO3oMDXp9ucd6H9bFis9m5D851pR0HbIrIfw5RKg1OWnPH\nli3YoAd3HP0o7P2+GRtddCfOC9R1r21wfi/guD9c6ZFlYlxpLbCi+ZNzzaaVXbdY94OT51zn3Ddj\nXTn1XWnVsC+Osu8uxvCbkJ+JfS42YXXjP8AREXZfjq0grXNsbBrELqw+lDrpK7EvsjoBy833uY5V\njkF+ZTdtxiIiv8fWlIoD5P0QG1q1MPWWKYqiZAcZL/SKoihKYmR6Z6yiKIqSICr0iqIoIUeFXlEU\nJeRUjZ0l9TRo0MA0b9483WYoiqJkFZ988smPxpiGsfJlhNA3b96cuXPnptsMRVGUrEJEvgmST103\niqIoIUeFXlEUJeSo0CuKooQcFXpFUZSQo0KvKIoScjIi6kZRFCVTmTq/lAfe+JJV67ewf0E+Q3q1\npLh9YewNMwgVekVRFA+mzi9l2JTP2LJ9JwCl67cwbMpnAFkl9uq6URRF8eCBN77cLfJlbNm+kwfe\n+NJji8xEhV5RFMWDVeu3VGh9pqJCryiK4sH+BfkVWp+pqNAripLxTJ1fSueR02kx9DU6j5zO1Pml\nlXLcIb1akl8tb491+dXyGNKrZaUcP1loZ6yiKBlNOjtEy/avUTeKoigpxK9DtDIEt7h9YdYJeyTq\nulEUJaMJS4doOlGhVxQlowlLh2g6UdeNoigZQ7SvUIf0armHjx6ys0M0nWiNXlGUjKCs07V0/RYM\ne3a6jujbhsKCfAQoLMhnRN82We83r0y0Rq8oSkbg1+k6e2h3FfYE0Bq9oigZgXa6pg4VekVRMgLt\ndE0dKvSKomQEYfkKNRNRH72iKBlBWL5CzURU6BVFyRjC8BVqJqKuG0VRlJATU+hF5CkRWS0iC13r\n6ovIWyKy1Plbz1kvIjJGRJaJyAIROTKVxiuKoiixCVKjHw/0jlg3FHjHGHMw8I6zDHAScLDzGww8\nlhwzFUVRlHiJ6aM3xvxXRJpHrD4V6Ob8PwGYCfzZWf9PY4wBPhCRAhFpbIz5LlkGK4oSmzBMaK0k\nj3h99I3KxNv5u6+zvhBY6cpX4qwrh4gMFpG5IjJ3zZo1cZqhKEokXkMJVNZkHUpAdu2C55+Hbt1g\nS2o/Ckt2Z6xEWWeiZTTGjDPGFBljiho2bJhkMxQldwnLhNZhZOr8UjqPeIeLz7idpYUHw7nnwrvv\nwtixKT1uvOGVP5S5ZESkMbDaWV8CNHXlawKsSsRARVEqhg4lUJ5McGVNnV/K/z04kYenP82RqyJe\nuiNGwODBULduSo4dr9C/DAwARjp/X3Ktv1JEJgEdgQ3qn1eUymX/gnxKo4h6rg4lkKqpCL1eHlHX\nby+lsP/lTFg2L/rO1q6F0aNh+PC47fEjSHjl88D7QEsRKRGRQViB7ykiS4GezjLAf4AVwDLgH8AV\nKbFaURRPdCiBPUmFK8urH+TWqZ/tsb7W0i+ofc6Z0LEjHbxEvozPPgMT1dOdMEGibs7xSOoRJa8B\n/pSoUYqixE+iQwlkgpujIsSyNxWuLK+Xx/MfrmSnMTRb9x3Xzn6O4kUzqRK9m3I3nzVrRZvxj8AJ\nJ8RtTyx0CARFCSHxDiWQKjdHqghibypcWV4viQY/r+HqOZM4a8FbVNu1M2qeMj7ftwVjThhI75su\nos2RTeK2JQgq9Iqi7MbPzZGOzstYLYsg9qZiKsLIl0e9XzZw+QeTGTDvVWrs3O677bf7FHJ/5/78\nr2NPbjzpsEq5rir0iqLsJlMidoK2LILYm8xRMctePqXrtyBA7V83c/FHUxk0dyq1t8W4Rk2bwvDh\nNBswgEeqVq70qtArirKbTInYCdqyCGpvMkbFdL98am7fygXzXuPyDyZTb+tG3+1+rbcPNW6/FS67\nDGrWTMiGeFGhVxRlN6lwc8RD0JZFovZWpOP5gTe+ZMfWrZy34C2umjOJRpvW+u+8bl0YMoQa114L\ntWsHsidVqNArirKbaG6OEw5tyANvfMl1L/yv0qJwKlJTj7Q3qH0V6njeuZOO773Gte89S7MNP/jv\nOD8frrkGhgyB+vVj2lEZiElR3GZFKCoqMnPnzk23GYqiRBAphmBrzCP6tkmp2FfGcTuPnB71ZVJY\nkM/sod3tgjEwdSrceit8/rn/DqtVg0svhVtugf32S4qNsRCRT4wxRbHy6cQjiqJ4kq5xc4rbFzKi\nbxsKC/IRrPgm++Xi6x4yBt58E44+Gvr29RV5U6UKDBwIS5bAww9XmshXBHXdKIriSTqjcFI9raCX\ne+j3G1bYj5fefTfmPkpPPJnCMQ/AYYelwsSkoUKvKD5k21eiySZTonBSQWRHbqsfVnDTexPptuyj\n2BufdBLcfTeFR2bHJHoq9IriQbZ9JZoKKjOqpbIps2PSxLfp/58nOWXxrNgbdekC994Lxx2XYuuS\niwq9oniQSV+JpotKi2pJB99+S/HYOykePx52+g9XQPv2VuB79QKJNu1GZqNCrygeZMpXoukmXl95\nxr4oV6+2478/+ihs2+af99BD4a67bIdsleyNXVGhVxQPwuyfrgwy7kW5fj2MGgUPPQSbN/vn/d3v\n4I474LzzIMBwBZnsogINr1QUT3Rc98TweiFW+oty82YYORJatIB77vEX+UaNbIjkl1/akMmAIp/p\nc/Sq0CuKB5URyx1m0v6i/PVXeOQROPBAGDbM1ui9qFfPunOWL4crr4QaNQIfJhvm6FXXjaL4kOpY\n7jCTzFEjK8SOHTBxonW9fPONf95ateC66+CGG6CgIK7DZZyLKgoq9IqipIxUvCg9/eG7dsGLL8Lt\nt8Pixb772JZXjYntTuLF3hdwSd9OFEeIfEV87tnQl6NCryhK1hA1ZPPFBew7ezrHPjUa5s/33X5X\nXh6Tj+jJ6E5n813dhrCDciGfFQ0LzZQRP/1QoVcynkyPaFAqj0h/eIeVCxny339ydEmMAccAzjmH\nc/bvzYdV99ljdWTIZ0XDQuN1UVXmfa1Cr2Q0Gf/RTZzoyys+yvzeh3+/jBv/+wzdvvok9kZ9+sDd\nd0Pbtnw09DXf/Ub+75Unkoq6qCr7vtaoGyWjyYaIhoqSDeF4mcox21YzduoIXp1wbWyR79YN5syB\nV16Btm2BYCGflREWWtn3tQq9ktFkQ0RDRQnjyyvlfPUVDBzIs3+7mJO/nO2ft0MHO8Tw9OlwzDF7\nJPmFfE6dX7p7jPrIQQ6S7XOv7Ps6IdeNiFwHXAwY4DPgQqAxMAmoD8wDzjfGxPjOWFGikw0RDRUl\njC+vlLmivvvOfuQ0bhxs315OgPegdWvrojn1VBDxtSlyPbCHK8UA4vwtTIFrrbLv67iFXkQKgauB\nVsaYLSLyL6Af8AdgtDFmkog8DgwCHkuKtUrOkQ0RDRUlbC+vlPib166F+++HMWNgS4wXYIsWcOed\ncM45kJcXyKZIuzqPnF6ulVUm8rtnm0oilX1fJ+q6qQrki0hVYC/gO6A7MNlJnwAUJ3gMJYcJ49ep\naf9iNMkk1RW1caOtlbdoAffd5y/yjRvDY4/ZmPnzztst8vHYVNmtrMq+r+Ou0RtjSkVkFPAtsAV4\nE/gEWG+M2eFkKwGy94lUMoKwfZ2ati9GU0RSRHLrVnj8cTsU8Jo1/nnr17dDGvzpT3Yi7iTYlI5W\nVmXe14m4buoBpwItgPXAv4GTomSNOvu4iAwGBgM0a9YsXjMUJStJ18srFb70hERy+3YYP966XkpK\n/PPWqWOHKrjuOqhbN6k2hdFF6CYR182JwFfGmDXGmO3AFOBYoMBx5QA0AVZF29gYM84YU2SMKWrY\nsGECZiiKEoRUhXXG5YratQsmTbIdqIMH+4t8zZpW4FesgOHDY4p8PDaF0UXoJpGom2+BTiKyF9Z1\n0wOYC8wAzsBG3gwAXkrUSEVREieW3zremn6FXFHGwKuvwq23woIF/juuWhUuvtjmLayY4MbjHgub\ni9CNGBPVsxJsY5G/AGcDO4D52FDLQn4Lr5wPnGeM+dVvP0VFRWbu3Llx26EouUpFXDEthr4W3Y+K\nre1Gui2SXqOdMQNuvhk++MA/nwj0729HnzzwwOQdP4SIyCfGmKKY+RIR+mShQq8oFScyhBCiC3TZ\nyyCazxogT4SdUXQgaaGFH30Et9wCb78dO29xsZ267/DDEz9uDhBU6HWsG0XJUoIMvhXtZeAmsibv\nJuHQwoUL4bbbYOrU2Hl79rRhlUcfndAhdQyh6OgQCIqSpQQJIYz2MiijrMOxMNljuyxfDuefD0cc\nEVvkO3WyQxW8+WZSRF7HEIqOCr2iZClBBt/yehkIMHtod4rbFybvA67SUrj8cjj0UDvDk59b+Igj\n7GBjc+bACSdU7Dge6BhC3qjQK0qWEkSgg7wMEg4t/PFHuPFGOOgg+9HTjh3eeQ8+GJ5/3k4Q0qeP\n7XhNEmEcQyhZqI9eUSLIFj9vkBDCoB8CxRVa+PPP8OCD9rdxo3/epk3tFH8DB9qwyRQQtjGEkokK\nvaK4yLaJTmIJdEqGW9iyBcaOhZEj4aef/PM2bGgjbi691H74lELC/nVrIqjQK4qLik4jlw0k7UOg\nbdvgqads+OOqqB+872ZjzVqM63Aar/fox5Vd21GcYpGH8I0hlExU6BXFhfp5o7Bzp/WrDx9uhyHw\nYUfNmjx15B8ZW3QaG/LrwJbyk29D6txjYf66NRFU6BXFhfp5XRhjwyNvuw0WLfLPW60aXHopfet0\nZcGuvfZIihXbn+nusTCgUTeK4iJsY8XHhTHw1lvQsSP07esv8lWqwIABsGQJPPwwn0WIfBmxYvs1\nDDK1aI1eUVzkvJ93zhzbeTpzZuy8Z5xhhxc+7LDdq4K0iNQ9Vvmo0CtKBDnp5/30Uyvwr70WO2/v\n3na4gqOOKpcUJPJF3WOVj7puFCWXWbLEzrXarl1ske/SBd59F15/ParIQ7CPr9Q9VvlojV5RcpGV\nK63b5emnbVSNH+3bwz332Jp8gC9Z0xLbr/iiQq94ki1fiCoVYPVqGDECHn3UxsX70bKljZk//XTb\n6Zogej+lDxV6JSoaApdZJCyS69fDqFHw0EOwebN/3mbN4C9/gfPO8x2uoCI26f2UXtRHr0RFQ+Ay\nh4SG39282Q5VcMAB1v3iJ/KNGsHDD1u/fYwxaSpqk95P6UWFXomKhsBlDnGJ5K+/wiOP2Kn4hg2D\ndeu889arZ905y5fDlVdCjRpJtylX76ep80vpPHI6LYa+RueR09M2Nr66bpSoaAhc+ok1BWBUkdyx\nw44Ff8cd8M03/geoVQuuuw5uuAEKCipkW0WFOxfvp0xyV2mNXomKhsClF7drxIs9RHLXLpg8Gdq0\ngQsv9Bf56tXhmmtsDf6uuyos8uWOHWB9Lt5PmeSuUqFXopLwZBRKQvhNAQgukTQGpk2DDh3gzDNh\n8WLvneblwaBBsHSp7ZRt1Chu+yoq3Ll4P2WSu0pdN4onOfmFaIbgJwYF+dUQgedGTaT5nGdp9/Vn\nsXfYr5+NpDnkkKTYF08sfK7dT5nkrlKhV5QkkqxYcS+RKMivxoErl3DV9PF0++qT2Dvq08cOV9C2\nbYVtiEWuCXdFyaSJUFToFSVJJLPzLZpItNqwiuteeYaen8+Kuf0HTQ/nb90v5Oxrzqa4rYpxOsik\nL4ATEnoRKQCeAA4HDHAR8CXwAtAc+Bo4yxjjE9ulKOEgmbNTuUVCvvmamz96gZP+9zaya5fvdp/u\ndzAPdL2A95q3AxG+zeKZscJAprR6Eq3R/w2YZow5Q0SqA3sBNwPvGGNGishQYCjw5wSPoygZT7I7\n34ob51FcMgWeHAfbt/vm/bJBMx487jzeOPiYPcajyYY4dR0aIfXELfQiUhfoCgwEMMZsA7aJyKlA\nNyfbBGAmKvRKDpC0zre1a+H++2HMGDsRtw/f7t2Iscefz9vtu/PT1vK1/UyPU8+kWPMwk0h45QHA\nGuBpEZkvIk+ISC2gkTHmOwDn777RNhaRwSIyV0TmrlmzJgEzFCUzSDhWfONGPv/TTWwsbAb33ecr\n8j/W2Ydbfn8F5984gWPuuJbbTj0iK+PUMynWPMwk4rqpChwJXGWM+VBE/oZ10wTCGDMOGAdQVFRk\nErBDUTKCuDvftm6Fxx/n1zvvptW6n/zz1q8Pw4bR4IoruGev8tP2ZZsLJJNizcNMIkJfApQYYz50\nlidjhf4HEWlsjPlORBoDqxM1UlGyhQp1vm3fDhMm2Pj2khJ8R5ipUweuv97+6tZN/NgZQibFmoeZ\nuF03xpjvgZUiUtY27AF8DrwMDHDWDQBeSshCRQkbu3bBpEnQujVccgmUlHhm3Vq1Ov/ocBqsWGHH\nr/EQ+WwlF4dGSAeJRt1cBTzrRNysAC7Evjz+JSKDgG+BMxM8hqKEA2PsdH233AILFvhm3V4ljxeO\n+D0PH3s2VZs25ZIGDSrJyMolk2LNw0xCQm+M+R9QFCWpRyL7VZTQMXMm3HwzvP++b7ZdCC+36sro\nLv35pt7+5FfLY0TIa7fZ6HLKNvTLWEVJJR9/bGvwb70VO29xMTPOvZIHlgur1m+hUGu3SpJQoVeU\nVLBwIdx2G0ydGjvviSfa2Z+OPpoeaHNYST4q9IoSkEBfcC5fbjtNn33W+uT96NQJ7r0XTjghZTYr\nCqjQK0ogYn7BWVpqR4l84gk7y5MfbdrYGnyfPnsMV6AoqUKFXlEC4PUF57gpH1H83Bw7P+vWrf47\nOegguPNOOPtsqKJz/iiVhwq9ogQg8kvN2r/+wsUf/x+DPp4K22J8xdmkCdx+OwwcCNWqpc5IRfFA\nhV5RAlD2BWeN7b9ywbzXuPzi3AogAAAXqklEQVTDydTf8rP/Rg0b2pDKyy6DmjUrx1BFiYIKvaIE\n4KbuLZh/x4NcNus59tu01j9z3bpw441w7bV26AJFSTMq9Irix86d8PzznDp8OKeuWOGfNz8frr4a\nbrrJDj4WQnTs+OxEhV5RomEMvPQS3HorLFrkn7daNRg82H4Y1bhx5diXBnTs+OxFu/4VxY0x8Pbb\n0LEjnHaav8hXqWI7WJcssVE3IRZ50LHjsxmt0StKGe+/b2vlM2bEznv66XDXXXDYYam3K0PQseOz\nFxV6JRQk5Dv+9FPronn11dh5e/e2H0YddVRyjp1F6Njx2Yu6bpSsp8x3XLp+C4bffMdT55f6b7hk\nCZxzDrRrF1vku3SBd9+F118vJ/JxHTsL0bHjsxcVeiXrqbDveOVKO+FHq1Z2AhA/2reH//wH/vtf\n6No18WNnMcXtCxnRtw2FBfkIUFiQz4i+bULZegkb6rpRsp7AvuPVq2HECHj0Udi2zXefG5sfSJ37\nR1hfvM9wBUGPHRb3jo4dn52o0CtZT0zf8fr1MGoUPPQQbN7su6+Sug15qEt/prU7kbsPakdxjDFp\ngvitNSxRSTfqulGyHi/f8dCuTWHkSDjgADtapI/Ir6lVwPATL6X7JeOY3OZENu0kkPsliN86l9w7\nSmaiNXol64mcd/R3tfP428a5tO07EH74wXfbDTVq8XinMxh/5Clsqb7neDRBwgaDzHmqYYlKulGh\nV0JBcftCits0gokT7cQf33zjm/+X6jV58qhT+cfRp/FzzdpR8wQNG4zlt9awRCXdqNAr2c+uXTBl\nip26b/Fi/7zVq8MVV9B1RxE/1irwzJbMsMEhvVru4aNP9v4VJRbqo1eyF2Ng2jTo0AHOPNNf5PPy\nYNAgWLoURo+mRqH3cAXJDhvUsEQl3WiNXslOZs2ywxXMmhU7b79+8Je/wCGH7F7lVctOlQBrWKKS\nThIWehHJA+YCpcaYPiLSApgE1AfmAecbY/yDlkNCWGKlU01C12nePCvw06bFztunjx2uoG3bcklB\nOlEVJSwko0Z/DfAFUNdZvg8YbYyZJCKPA4OAx5JwnIxGY6WDEfd1+uILOx3f5MmxD3L88XDvvXDs\nsb7ZtJat5AoJ+ehFpAlwMvCEsyxAd6DsaZwAFCdyjGwh12Klp84vpfPI6bQY+hqdR04PPLZLha/T\n11/DhRfC4YfHFvmiInjjDTv6ZAyRV5RcItEa/UPATUDZfGn7AOuNMTuc5RIgapVJRAYDgwGaNWuW\noBnpJ5dipRNpvQS+Tt9/bz9y+vvfYft2f4Nat7ZDBhcXg0iwk1CUHCLuGr2I9AFWG2M+ca+OktVE\n294YM84YU2SMKWrYsGG8ZmQMXjHRYYyVTqT1EvM6rV0Lw4axo0ULO5mHn8i3aAHPPGOHGT7tNBV5\nRfEgEddNZ+CPIvI1tvO1O7aGXyAiZS2FJsCqhCzMEsIyhGsQl0wirRfP4QqOa8Lnf/ozGwubwciR\nVN261XsnjRvbgckWL4bzzrOhk3Gch6LkCnG7bowxw4BhACLSDbjRGNNfRP4NnIEV/wHAS0mwM+MJ\nQxRHUJdM0C89/aJrytY3r5XHmA0fcsgp51Fj3U++9m3Ir8Ped94OV1wBe+2V8HkoSq6Qijj6PwOT\nRORuYD7wZAqOkZFkexSHn0vGfV5BvvSMJbbFbRrB+PE2vr2kxNeuTdXzebKomCePLmbBjWcl7TwU\nJVdIitAbY2YCM53/VwBHJ2O/2UCYYueDumSCtF68xHbU619Q/OUsGyq5dKmvPVurVuef7U/msU5n\nsG6vvSkM2N9RGR3jYSp3Jfzol7EJEDYXQUUG34rVeiknqsbQffnH3DjrGVj9la8d26vk8a8jejLm\n2H78UKcBULH+jlQPIha2clfCj451kwBhi51PZoeyW1Q7fbuAFycO4akX76SVj8jvQpjS+gR6XPw4\nt/S6ktWOyFd0bJhUd4yHpdy1wzp30Bp9AoQtdj6ZHcpDerXk2Ucmc9X0CXT9en6Agxcz49wr+ety\nYdX6LRQGOLaX+yTVHeNhKHdtleQWKvQJEMZxxpPSobxwIcV33kbx1Kmx8554oh2PpmNHegA9Ah4i\nZmdvCjvGw1Du2mGdW6jrJgHCEjufNJYvh/PPhyOOgFgi36kTvPMOvPUWdOxY4UOl030StNwz2TUS\nhlaJEhyt0SdAGGLnk8KqVXYIgieegB07/PMecYQd2uDkk0Ek7uiVdApVkHLPdNdIGFolSnBU6BMk\n22PnE+LHH+G+++xQBX5fsgIcdJCNme/XD6rYhmQiYphuoYpV7pnuGtFZr3KL0LluMrm5HBp+/tmK\n9gEHwKhR/iLfpAmMGweffw7nnrtb5CEx90umu80y3TWis17lFqGq0VdWczlnP5bZsgXGjoWRI+En\n/+EKaNDAThBy2WVQs2bULImIYaa7zdLd4ghCTrdGc4xQCX1lNJcz3feaErZtg6eesn74Vf5j1G2q\nUYtvB11Bq5G3QZ06vnkTFcNMFip1jSiZRKhcN5XRXA7LxzKB2LnTDgN86KFw+eW+Ir+lag0e73g6\nnS99gtPrd2fqsp9j7j7T3S+JoK4RJZMIVY2+MprLme57TQrGwEsvwa23wqJFvlm351Xluba9eOSY\ns1lTu76zMlgrKtPdL4mSyS0OJbcIldAno7kcy/+eDb7XuDHGxrbffDN8/LFv1p1ShZI+Z9C/sBcl\nezcqlx70xadiqCipJ1RCn2gNMYj/PbS+1/fft52nM2bEzPpay8482OU8VjVuTs1qVeCX8rNAVWRs\nekVRUkuohB4SqyEG6cxNh7shpSK5YIEV+FdfjZl1ZoujGNX1fBbud5BdsX0nNapWIb9aXkJj0yuK\nklpCJ/SJUJHx2CtLoFImkkuWwPDhMGlS7LxdunB2sz582PTwckkbtmxn9Nnt4hqbPlM+HlKUsKNC\n7yIT/e9JF8mVK+HOO+Hpp21UjR/t29vhCnr3puS+GeBxbSo8Nn2M9YqiJJdQhVcmSiaG+yVNJFev\nhuuus0MRPPGEv8i3bAn//jfMnQsnnQQiCV0brxdlKDqwFSUL0Bq9i0wM90u4lbF+Pfz1rzB6NGze\n7J+3WTO44w47AmXVPW+NoNcmWn9CaDuwFSVLEGNMum2gqKjIzJ07N91mZCSRPnqwIhnz45vNm+1g\nY/fdB+vW+R+kUSMbM3/JJVCjRkpshcx6gSpKGBCRT4wxRbHyaY0+w6lwK+PXX+Ef/7C+9e+/9995\nQQHcdBNcfTXUqpWwrX79CbOHdldh90BDT5VUo0KfBQSK8tmxAyZOtK6Xb77xz1urFlx7Ldx4oxX7\nJKGdrhVHQ0+VykA7Y7OdXbtg8mRo0wYuvNBf5KtXh2uusTNB3X13UkUetNM1HnJq7CQlbcQt9CLS\nVERmiMgXIrJIRK5x1tcXkbdEZKnzt17yzFV2YwxMmwYdOsCZZ8Lixd558/Jg0CBYuhQeesj65FNA\nJkYtZTraClIqg0RcNzuAG4wx80SkDvCJiLwFDATeMcaMFJGhwFDgz4mbquxm1iz7NeusWbHznn22\njZs/5JA9VqfCL5yJUUuZTiZ+u6GEj7iF3hjzHfCd8/9GEfkCKAROBbo52SYAM1GhTw7z5tnomNdf\nj523Tx87fny7duWSUukX1kHKKoaGniqVQVJ89CLSHGgPfAg0cl4CZS+DfT22GSwic0Vk7po1a5Jh\nRnj54gvrnjnqqNgif/zxMHs2vPJKVJEH9QtnEjpuvVIZJBx1IyK1gReBa40xP4tIoO2MMeOAcWDj\n6BO1I5R8/bWdm/Wf/7Sdrn4UFcG998KJJ0KMMlC/cGahrSAl1SRUoxeRaliRf9YYM8VZ/YOINHbS\nGwOrEzMxB/n+e7jqKutXHz/eX+RbtYIpU+Cjj6Bnz5giDxodoyi5Rtw1erFV9yeBL4wxD7qSXgYG\nACOdvy8lZGEusW4d3H8/jBkDv/zin7dFC1vbP/dcG1XjgQ5JoB8kKUrcQyCISBdgFvAZUFblvBnr\np/8X0Az4FjjTGLPWb185PwTCpk1W3O+/HzZs8M/buDHcfjtcdJGNi/dBhyRIYAgJRckCgg6BkFNj\n3WRczW7rVvj7361vfXUMD1f9+jBsGFxxBey1V6Dddx45PWroXmFBPrOHdo/H4qxDr4ESZnSsmwgy\n6lPzHTtgwgTrelm50j9v7dpwww12iOG9967QYbTTVa+BokAODYGQESGFu3bBCy/YDtSLL/YX+Ro1\nrMCvWGHHr6mgyIN2uoJeA0WBHBL6tNbsjLFzsh55JPTrZ4ci8KJqVbj0Uli2DEaNgoYN4z6sDkmg\n10BRIIeE3qsGZ7B+3KnzS5N6vKnzS+k8cjr9zh3BZ80Ph1NOgU8/9d5ABPr3tx9HPf44NGmSsA36\nMY5eA0WBHOqMjRZ94SaZkRhT55fy7COTuWr6BLp+PT/2Bn/8ox1Nsk2bhI+tKErukFOdsUGiadwD\nbkWLwkhowm03ixZR7/zL+Pei92Ln7dHDThDSsWNix1QURfEh6103ZTX10vVbMPwWTRPNFVPcvpDZ\nQ7vj9e1oQv76FSvgggugTRuOjyXynTrBO+/A22+ryCuKknKyXujjiaZJaiRGaSlcfjm0bAnPPGM7\nXj1Ytl8LeOklmDMHumsMt6IolUPWC3080TRJicT48UcYMgQOOsh2nu7Y4Zn1q3qNuaH4zyx89V3r\njw848JuiKEoyyHoffTwTNyQ0QcbPP8Po0fDXv8LGjb5ZV9dtwOhj+jH7uFO4/g+tNdIjA8i4r6MV\npRLIeqGPd4CuCg8Nu2ULPPoojBgBP/3kn7dBA7j5Zva9/HJG1KwZ/BhKSsmor6MVpRLJWqF318z2\nzq9GzWpVWP/L9uTX0rZvh6eestPxrVrln7duXbjxRrj2WqhTJ6bdWqOsXPz6c7QMlDCTlUIfWTNb\nv2U7+dXyGH12u+Q9sDt3wqRJMHw4LF/unzc/344ff9NNsM8+ge2OrFEm8hIIsm2uv2R03BslV8lK\noU9pzcwYGxlz222wcKF/3mrVYPBgO1F348Yxdx0rQihet0IQl4S6LXQibiV3ycqom5TUzIyxce2d\nOsFpp/mK/E6pwrennAlLlsAjjwQSeT/7Vq3fktCga0G2zYhB3dKMjnuj5CpZKfRJH5Hw/fftV6o9\ne9op+Xz4zyHH0uuiR+jV9iKmrqtWocP42Z3IyyvItuq20HFvlNwlK103SZsKb8ECuPVWeOWVmFln\ntjiKUV3PZ+F+B9kVcbiK/Oz2GpohyMsriEtC3RYWnYhbyUWyUugTioMHO0zw8OHw/POx83bpwtnN\n+vBh08PLJVW0NhzL7siXgGB96Z1HTt+dL945YHNtnlhFUX4jZ0avBOxEH3fdZcMld0YfxXI37drZ\nAcdOOonO982olOnoykS8dP0WBDuEchn51fI4/ahCXvykNO45YHM96kZRwobOGetmzRr7odOjj8Kv\nv/rnbdnSvgxOPx2q2C6Myp5g2mue0zwRdkYpr8gXjgq6ouQGOTVMsScbNtihCkaPhk2b/PM2a2bd\nORdcYGd5cpGwq6iCeLmEool8ZH4No1QUJZJwCv3mzTbs8b77YN06/7z77ms7ZAcPtvO0elCZnXhe\nHadeNXp3h6p+/akoSiRZGV7pybZtMHasHVFy6FB/kS8ogHvvhRUrmNqlL51Hz6bF0NdSMq1gRfGK\n9z6nY9OYceAaRqkoSiQpqdGLSG/gb0Ae8IQxZmQqjrObnTth4kS44w74+mv/vLVqwTXX2CGGCwoy\n0tXh5yoq+l19XxeShlEqihJJ0jtjRSQPWAL0BEqAj4FzjDGfe20Td2fsrl0wZQrcfrudVNuP6tXt\nBCHDhkGjRrtXe3V8JjOipjI7Ryu747iy0Y5mRfmNdHbGHg0sM8ascAyZBJwKeAp9hTEG3njDjjEz\nb55/3ipV4MIL7cugWbNyyal2dVR2i6GyO44rk0xsfSlKNpAKoS8EVrqWS4ByE6OKyGBgMECzKALs\niTHQuze8+WbsvGedZYcXbun9UZCfqyMZtcd0dI6G9etP7WhWlPhIRWdstHnyyvmHjDHjjDFFxpii\nhg0bVmDvAoeX/0p1D04+GebPhxde8BV58O74POHQhoEnHfdDO0eTh15LRYmPVAh9CdDUtdwEiDFj\nRwUZOhRq1y6//vjj4b334NVX7ZetAfAa6GrG4jVJGe0x6QOw5TB6LRUlPlIh9B8DB4tICxGpDvQD\nXk7qERo2hOuv/225qMi6cmbMgM6dK7y74vaFzB7ana9Gnszsod0pbl+YtNqjDo2bPPRaKkp8JN1H\nb4zZISJXAm9gwyufMsYsSvZxuP56K+zXXQfFxdalk0SSFaYY5s7RykavpaLER26MdRMHYQ9TVBQl\n+9GxbhJEa4+KooSF0At9IiGSYQ1TVBQltwi10OsHNoqiKCEX+rB/YKPDASiKEoRQC32YP7DR1oqi\nKEEJ1zDFEYT5Axu/1oqiKIqbUAt9mD+wCXNrRVGU5BJqofca3iAMro0wt1YURUkuofbRQ+aHSMbb\noTqkV8uoH3SFobWiKEpyCb3QZzKJdKgm84Mujd5RlHCjQp9GEg3/TEZrRaN3FCX8hNpHn+lkQoeq\nRu8oSvhRoU8jmdChmgkvG0VRUosKfRrJhPDPTHjZKIqSWlTo00gmhH9mwstGUZTUop2xaSbd4Z86\nHLOihB8VeiXtLxtFUVKLum4URVFCjgq9oihKyFGhVxRFCTkq9IqiKCFHhV5RFCXkiDEm3TYgImuA\nb+LcvAHwYxLNyRZy8bxz8ZwhN887F88ZKn7evzPGNIyVKSOEPhFEZK4xpijddlQ2uXjeuXjOkJvn\nnYvnDKk7b3XdKIqihBwVekVRlJATBqEfl24D0kQunncunjPk5nnn4jlDis476330iqIoij9hqNEr\niqIoPqjQK4qihJysFnoR6S0iX4rIMhEZmm57UoGINBWRGSLyhYgsEpFrnPX1ReQtEVnq/K2XbluT\njYjkich8EXnVWW4hIh865/yCiFRPt43JRkQKRGSyiCx2yvyYHCnr65z7e6GIPC8iNcNW3iLylIis\nFpGFrnVRy1YsYxxtWyAiRyZy7KwVehHJA8YCJwGtgHNEpFV6rUoJO4AbjDGHAZ2APznnORR4xxhz\nMPCOsxw2rgG+cC3fB4x2znkdMCgtVqWWvwHTjDGHAm2x5x/qshaRQuBqoMgYcziQB/QjfOU9Hugd\nsc6rbE8CDnZ+g4HHEjlw1go9cDSwzBizwhizDZgEnJpmm5KOMeY7Y8w85/+N2Ae/EHuuE5xsE4Di\n9FiYGkSkCXAy8ISzLEB3YLKTJYznXBfoCjwJYIzZZoxZT8jL2qEqkC8iVYG9gO8IWXkbY/4LrI1Y\n7VW2pwL/NJYPgAIRaRzvsbNZ6AuBla7lEmddaBGR5kB74EOgkTHmO7AvA2Df9FmWEh4CbgJ2Ocv7\nAOuNMTuc5TCW9wHAGuBpx2X1hIjUIuRlbYwpBUYB32IFfgPwCeEvb/Au26TqWzYLvURZF9pYURGp\nDbwIXGuM+Tnd9qQSEekDrDbGfOJeHSVr2Mq7KnAk8Jgxpj2wmZC5aaLh+KVPBVoA+wO1sK6LSMJW\n3n4k9X7PZqEvAZq6lpsAq9JkS0oRkWpYkX/WGDPFWf1DWVPO+bs6XfalgM7AH0Xka6xLrju2hl/g\nNO0hnOVdApQYYz50lidjhT/MZQ1wIvCVMWaNMWY7MAU4lvCXN3iXbVL1LZuF/mPgYKdnvjq28+bl\nNNuUdBzf9JPAF8aYB11JLwMDnP8HAC9Vtm2pwhgzzBjTxBjTHFuu040x/YEZwBlOtlCdM4Ax5ntg\npYi0dFb1AD4nxGXt8C3QSUT2cu73svMOdXk7eJXty8AFTvRNJ2BDmYsnLowxWfsD/gAsAZYDt6Tb\nnhSdYxdsk20B8D/n9wesz/odYKnzt366bU3R+XcDXnX+PwD4CFgG/BuokW77UnC+7YC5TnlPBerl\nQlkDfwEWAwuBZ4AaYStv4HlsH8R2bI19kFfZYl03Yx1t+wwbkRT3sXUIBEVRlJCTza4bRVEUJQAq\n9IqiKCFHhV5RFCXkqNAriqKEHBV6RVGUkKNCryiKEnJU6BVFUULO/wM4Bza5nSX2awAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1172363c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n",
    "\n",
    "def generate_dataset(num_samples=100, noise=100):\n",
    "    '''Generates a random number of samples to act as a dataset for linear regression.'''\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(num_samples):\n",
    "        x.append(i)\n",
    "        if noise == 0:\n",
    "            added_noise  = 0\n",
    "        else:\n",
    "            upper_noise_limit = noise / 2\n",
    "            lower_noise_limit = (upper_noise_limit) * -1\n",
    "            added_noise = np.random.rand() *  np.random.randint(lower_noise_limit, upper_noise_limit)\n",
    "        y.append(i + added_noise)\n",
    "    return x, y\n",
    "\n",
    "def show_best_fit(x, y):\n",
    "    '''Plots a line of best fit on top of the data points'''\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color='red', linewidth=5)\n",
    "    m, b  = np.polyfit(x, y, 1)\n",
    "    plt.title(\"f(x) = {}x + {}\".format(m, b))\n",
    "    \n",
    "x, y = generate_dataset()\n",
    "show_best_fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Using Mulitple Regression for the Boston Housing Prices Dataset</h2></center>\n",
    "\n",
    "In the example above, we were able to graph our findings because we only used 1 predictor.  Although we can't graph really graph a regression that uses more than 2 predictors, we can still fit the regression and use it for making predictions.  In this problem set, that's exactly what we'll do.\n",
    "\n",
    "<center><h3>About the Data Set</h3></center>\n",
    "\n",
    "The `Boston_Housing_Prices` dataset is a famous dataset usually used for learning or benchmarking regression techniques.  This dataset contains 14 variables, one of which (MEDV, median value of home) we'll use as our target. All other variables are continuous, with the exception of 1 being binary (if the house shares a border with the Charles River).  For a full description of each predictor, you can [follow this link](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names), or just examine the `.DESC` attribute of the object `load_boston()` returns.  \n",
    "\n",
    "<center><h3>Your Task</h3></center>\n",
    "\n",
    "For this assignment, your task is to:\n",
    "\n",
    "1. Read in the dataset using sklearn's `load_boston()` function (found in `sklearn.datasets`)\n",
    "<br>\n",
    "<br>\n",
    "2. Perform some basic exploratory data analysis to get a feel for the data.  Graph some stuff!\n",
    "<br>\n",
    "<br>\n",
    "3. Create a **_correlation heatmap_** to check to see how highly correlated our predictor variables are (Remember, if our predictors are highly correlated, this is bad.) . \n",
    "<br>\n",
    "<br>\n",
    "4. Split the data into training and testing, or use K-Fold Cross Validation.  Your choice.  \n",
    "<br>\n",
    "5. Fit a regression model.  Examine the results.  If any of the predictors have a P-value of greater than 0.05, remove that predictor from your dataset and rerun the regression.  Repeat until you have a well-fit regression model. \n",
    "<br>\n",
    "<br>\n",
    "6. Make predictions on your test set (X_test) and see how well it compares to the actual targets (y_test) from the test set.  \n",
    "\n",
    "\n",
    "<center>**_Step 1: Import the Data_**</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the load_boston() function from sklearn.datasets\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "# Create a dataframe object out of the data.  Use boston.data for the data, and pass boston.feature_names \n",
    "# for the columns parameter\n",
    "df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "\n",
    "# Store the data in boston.target in the targets variable\n",
    "targets = boston.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**_Step 2: Exploratory Data Analysis_**</center>\n",
    "\n",
    "Be sure to familiarize yourself with this dataset enough to answer the following questions:\n",
    "\n",
    "1.  Are there any null values in this dataset?\n",
    "2.  Are there any outliers in this dataset?\n",
    "3.  What are the descriptive statistics of each predictor?\n",
    "\n",
    "If you run across any null values or outliers, deal with them, and make note of your strategy for doing so. \n",
    "\n",
    "**_A Note on the Data in this Data Set:_**: If you examine the predictors, you'll likely notice one of the predictors, B, is problematic for a number of reasons.  This predictor is a measure of how much of the neighborhood was occupied by Black residents at the time.  This is not a majorly predictive attribute, but even if it was, it it brings up questions about whether we _should_ use data on things such as race, gender, etc in our models.  We have chosen to leave this attribute in the dataset because this is what the actual dataset contains, but with the caveat that we will be using this to have a larger discussion about data ethics in class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO  LSTAT  \n",
       "0       15.3   4.98  \n",
       "1       17.8   9.14  \n",
       "2       17.8   4.03  \n",
       "3       18.7   2.94  \n",
       "4       18.7   5.33  \n",
       "5       18.7   5.21  \n",
       "6       15.2  12.43  \n",
       "7       15.2  19.15  \n",
       "8       15.2  29.93  \n",
       "9       15.2  17.10  \n",
       "10      15.2  20.45  \n",
       "11      15.2  13.27  \n",
       "12      15.2  15.71  \n",
       "13      21.0   8.26  \n",
       "14      21.0  10.26  \n",
       "15      21.0   8.47  \n",
       "16      21.0   6.58  \n",
       "17      21.0  14.67  \n",
       "18      21.0  11.69  \n",
       "19      21.0  11.28  \n",
       "20      21.0  21.02  \n",
       "21      21.0  13.83  \n",
       "22      21.0  18.72  \n",
       "23      21.0  19.88  \n",
       "24      21.0  16.30  \n",
       "25      21.0  16.51  \n",
       "26      21.0  14.81  \n",
       "27      21.0  17.28  \n",
       "28      21.0  12.80  \n",
       "29      21.0  11.98  \n",
       "..       ...    ...  \n",
       "476     20.2  18.68  \n",
       "477     20.2  24.91  \n",
       "478     20.2  18.03  \n",
       "479     20.2  13.11  \n",
       "480     20.2  10.74  \n",
       "481     20.2   7.74  \n",
       "482     20.2   7.01  \n",
       "483     20.2  10.42  \n",
       "484     20.2  13.34  \n",
       "485     20.2  10.58  \n",
       "486     20.2  14.98  \n",
       "487     20.2  11.45  \n",
       "488     20.1  18.06  \n",
       "489     20.1  23.97  \n",
       "490     20.1  29.68  \n",
       "491     20.1  18.07  \n",
       "492     20.1  13.35  \n",
       "493     19.2  12.01  \n",
       "494     19.2  13.59  \n",
       "495     19.2  17.60  \n",
       "496     19.2  21.14  \n",
       "497     19.2  14.10  \n",
       "498     19.2  12.92  \n",
       "499     19.2  15.10  \n",
       "500     19.2  14.33  \n",
       "501     21.0   9.67  \n",
       "502     21.0   9.08  \n",
       "503     21.0   5.64  \n",
       "504     21.0   6.48  \n",
       "505     21.0   7.88  \n",
       "\n",
       "[506 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Exploratory Data Analysis here!\n",
    "df.drop(\"B\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Step 3: Create a Correlation Heatmap_**\n",
    "\n",
    "In this step, we're going to use the `seaborn` package to crete a correlation heatmap, so that we can see how much correlation there is between each predictor variable.  \n",
    "\n",
    "The code for building a heatmap can seem a bit intimidating, but don't worry--it's nothing you need to memorize.  You'll find great instructions and examples [in this tutorial](http://www.neural.cz/dataset-exploration-boston-house-pricing.html).  You can also look at the [seaborn docs for heatmaps](https://seaborn.pydata.org/generated/seaborn.heatmap.html).  \n",
    "\n",
    "**Note:** The tutorial uses an older deprecated method that still works, and is likely the easier route here.  In the current version of `seaborn`, you'll find it inside `sns.linearmodels`.  \n",
    "\n",
    "Your heatmap should look something like this when completed:\n",
    "\n",
    "<center><img src='img/heatmap.png' height=60% width=60%></center>\n",
    "\n",
    "\n",
    "<center><h3>How to Interpret a Correlation Heatmap</h3></center>\n",
    "\n",
    "This graph can look a little overwhelming at first, but it's not as confusing as it seems.  The whole idea of a correlation heatmap is to line up the columns of the dataset into a grid, and make it easily readable to see what correlates with what.  Each combination of columns is represented as a unique cell in the grid.  \n",
    "\n",
    "You might notice that the diagonal through the center of the grid is our key.  This makes sense, because if we plotted each at the same position on the x-axis and y-axis (for instance, CRIM would be the first column and the first row), then the box that currently says CRIM would be where we see a measure of CRIM's correlation with itself.  Since any variable's correlation with itself will always be 1, this is a nonsense column, this makes it a great place to put the variable names.  \n",
    "\n",
    "The best way to read a correlation heatmap is to find the variable you're looking for, and realize that the both row and column extending from it all correspond to that variable.  Anything below the diagonal is going to be mirrored across the diagonal as well.  In the example above, 2 cells down from `INDUS` and 2 cells right of `INDUS` are the exact same thing--the correlation between `INDUS` and `NOX`.  Cells below the diagonal represent this information through color, whereas the cells above the diagonal just give us the actual value.  \n",
    "\n",
    "Create the heatmap, and then use it to determine which variables have the highest correlation with other variables.  This will be important to help you figure out which columns to remove when tuning your regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a heatmap using sns.linearmodels.corrplot(), or sns.heatmap().  If you use .heatmap(), you'll need to \n",
    "# create a correlation matrix by using np.corrcoef() on your data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Steps 4 - 6: Creating and Tuning your Regression</h3></center>\n",
    "\n",
    "For the remainder of the notebook, you'll be creating, fitting, and tweaking your Linear Regression.  \n",
    "\n",
    "**_TASKS_**:\n",
    "\n",
    "1. Split your data into training and testing sets using train_test_split (alternatively, you can opt to use K-Fold Cross Validation. Either is available in `sklearn`). \n",
    "<br>\n",
    "<br>\n",
    "2. Use a [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object from SKlearn, and fit it on your training data.  If you run into problems, use the docs and/or Stack Overflow to help!\n",
    "<br>\n",
    "<br>\n",
    "3. Use it to predict for the data in `X_test`, and plot them against the actual values contained in `y_test`.\n",
    "<br>\n",
    "<br>\n",
    "4. Examine your R<sup>2</sup> value.  How much of the variance does your model explain?\n",
    "<br>\n",
    "<br>\n",
    "5. Optional: Tweak your model.  See if you can increase your R<sup>2</sup> by dealing with highly correlated predictors, normalizing your continuous data, etc.  Try whatever data science techniques you've learned so far (or that you can discover on the internet) to make this model more accurate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
